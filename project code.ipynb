{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/andmeteadus/projekt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1EAfnBtfYq3",
        "outputId": "f8298de6-4302-46a9-aa74-b2eddaa88abb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import math\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "d52pBXLrftJh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read data from files and preprocess the data"
      ],
      "metadata": {
        "id": "lYtdTgqJMU5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tdOMxSlGdXKu"
      },
      "outputs": [],
      "source": [
        "#read datasets and merge train and test datasets together (into dataset called traintest)\n",
        "uniqueTypes = pd.read_csv('unique_types.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sampleSubmission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "train[\"dataset\"] = \"train\"\n",
        "test[\"dataset\"] = \"test\"\n",
        "\n",
        "traintest = pd.concat([train, test])\n",
        "traintest['parameter_unit'] = traintest['parameter'].str.cat(traintest['unit'], sep = \"_\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the dataframe, making 1-hot vectors of values in given column that appear at least the number of times given as the limit\n",
        "def make1hotsOfColumnValues(df, column, limit):\n",
        "    replaceable = []\n",
        "    for el in df[column].unique():\n",
        "        if df[df[column] == el].shape[0] < limit:\n",
        "            replaceable.append(el)\n",
        "    df[column] = df[column].replace(replaceable, 'PihlaJaHannaProjekt')\n",
        "    newDf= pd.get_dummies(df[[column]], columns=[column]) #dataframe that contains the new columns only\n",
        "    df=pd.concat([newDf, df], axis=1)\n",
        "    df=df.drop([column], axis=1)#remove the original column\n",
        "    return df"
      ],
      "metadata": {
        "id": "ha0nIL-jE-Xu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removes short words from the given list and returns the list but with all words lowercased\n",
        "def removeShortWordsAndLower(words):\n",
        "  newWords = []\n",
        "  for w in words:\n",
        "    if len(w)>1:\n",
        "      newWords.append(w.lower())\n",
        "  return newWords\n",
        "\n",
        "\n",
        "#returns list of most frequent words in one column. (these words can appear in text of column values)\n",
        "def findFrequentWords(df, col, howMany):\n",
        "  unnecessarySymbols=\"[ ,.;:()\\[\\]\\\"\\'\\\\n]+\"\n",
        "  wordFreqs = dict()\n",
        "  for index, row in df.iterrows():\n",
        "    colValue = row[col]\n",
        "    if(type(colValue)!=float or not math.isnan(row[col])):\n",
        "      rowWords = re.split(unnecessarySymbols, row[col])\n",
        "      rowWords = removeShortWordsAndLower(rowWords)\n",
        "      for w in rowWords:\n",
        "        if(w not in wordFreqs):\n",
        "          wordFreqs[w] = 0\n",
        "        wordFreqs[w] += 1\n",
        "  \n",
        "  freqValues = [k for k, v in sorted(wordFreqs.items(), key=lambda item: item[1], reverse = True)]\n",
        "  return freqValues[:howMany]\n",
        "\n",
        "\n",
        "#makes new columns for most frequent words in given column and fills the columns with 1 and 0 values\n",
        "def makeBinaryColumnsOfFrequentWords(df, col, howMany):\n",
        "  freqWords = findFrequentWords(df, col, howMany)\n",
        "\n",
        "  for w in freqWords:\n",
        "    wordInColumnBools = [] #booleans as integers\n",
        "    for index, row in df.iterrows():\n",
        "      isInColumn = False\n",
        "      colValue = row[col]\n",
        "      if(type(colValue)!=float or not math.isnan(row[col])):\n",
        "        isInColumn = w in colValue.lower()\n",
        "      wordInColumnBools.append(int(isInColumn))\n",
        "\n",
        "    newCol = col+\"_\"+w\n",
        "    df.insert(2, newCol, wordInColumnBools)\n",
        "\n"
      ],
      "metadata": {
        "id": "JcjPfW-7nIVE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make list of unique types\n",
        "uniqueTypesList = []\n",
        "for index, row in uniqueTypes.iterrows():\n",
        "  uniqueTypesList.append(row[\"type\"])"
      ],
      "metadata": {
        "id": "gcmgjBkWKWRO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_1hot_with_limits = [('name', 8), ('material', 4), ('commentary', 5), ('event_type', 10), ('location', 4), ('start', 10), ('end', 8), ('country_and_unit', 10), ('before_Christ', 1), ('participants_role', 1), ('participant', 3), ('text', 3), ('class', 1), ('technique', 10), ('parameter_unit', 1), ('value', 2), ('museum_abbr', 10), ('musealia_mark', 1), ('musealia_additional_nr', 1), ('collection_mark', 11), ('collection_additional_nr', 1), ('legend', 22), ('initial_info', 7), ('damages', 4), ('state', 3), ('color', 1), ('additional_text', 11)]\n",
        "columns_containing_useful_free_text = ['name', 'commentary', 'text', 'technique', 'legend', 'initial_info', 'additional_text']\n",
        "\n",
        "columns_to_drop = ['full_nr', 'parish', 'element_count', 'ks', 'musealia_seria_nr', 'musealia_queue_nr', 'collection_queue_nr', 'is_original', 'parameter', 'unit']\n",
        "\n",
        "for col in columns_to_drop:\n",
        "    traintest = traintest.drop(col, axis=1)"
      ],
      "metadata": {
        "id": "RkGHH-FVHYIk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "howMany = 50\n",
        "\n",
        "#make binary columns of frequent words in columns listed in columns_containing_useful_free_text\n",
        "for i in range(len(columns_containing_useful_free_text)):\n",
        "    print(columns_containing_useful_free_text[i]) # to keep track of how far we are\n",
        "    makeBinaryColumnsOfFrequentWords(traintest, columns_containing_useful_free_text[i], howMany)\n",
        "\n",
        "# estimated running times for this block with different howMany values\n",
        "# howMany = 10 - 1 min\n",
        "# howMany = 30 - 4 min\n",
        "# howMany = 50 - 6 min\n",
        "# howMany = 100 - 13 min"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W_bcTX2IjwA",
        "outputId": "833de960-663c-4729-e285-901b09b0934e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name\n",
            "commentary\n",
            "text\n",
            "technique\n",
            "legend\n",
            "initial_info\n",
            "additional_text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make 1-hot vectors of frequent values in columns listed in columns_to_1hot_with_limits\n",
        "#this block runs 1-2 minutes\n",
        "for i in range(len(columns_to_1hot_with_limits)):\n",
        "    traintest = make1hotsOfColumnValues(traintest, columns_to_1hot_with_limits[i][0], columns_to_1hot_with_limits[i][1])"
      ],
      "metadata": {
        "id": "FhOwt0ZmIm4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traintest = traintest.fillna(0)"
      ],
      "metadata": {
        "id": "6WTf6j_iR0Ou"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save preprocessed data to file or read it from file"
      ],
      "metadata": {
        "id": "RrBj91DPYiVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"preprocessedData.csv\"\n",
        "#uncomment as needed\n",
        "\n",
        "#traintest.to_csv(filename, index=False) #saves data to file\n",
        "#traintest = pd.read_csv(filename) #loads data from file"
      ],
      "metadata": {
        "id": "N9N9olbHPWjR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Separating original trainset and testset"
      ],
      "metadata": {
        "id": "YT1zC5n2m1SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = traintest[traintest[\"dataset\"]==\"train\"]\n",
        "test = traintest[traintest[\"dataset\"]==\"test\"]\n",
        "\n",
        "train = train.drop(\"dataset\", axis=1)\n",
        "test = test.drop(\"dataset\", axis=1)"
      ],
      "metadata": {
        "id": "jrcAUR_cUaWs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnsX=list(train.columns)\n",
        "columnsX.remove(\"type\")"
      ],
      "metadata": {
        "id": "DlnXEiOAi02R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating validation set and using it to find the best model\n"
      ],
      "metadata": {
        "id": "3DPt_0pA8sDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separating trainset into training and validation data\n",
        "x_train_val, x_val, y_train_val, y_val = train_test_split(train[columnsX], train[[\"type\"]], random_state=1, train_size=0.75)"
      ],
      "metadata": {
        "id": "hRdWvKuIkJf8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in this block you can test different models and find their accuracy on the validation set\n",
        "dtc =  DecisionTreeClassifier(random_state=8, max_depth=100, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0000001, max_features=None, criterion=\"entropy\") #accuracy 0.904\n",
        "rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=\"log2\") #accuracy 0.909\n",
        "#gnb = GaussianNB() #accuracy 0.3\n",
        "#knn = KNeighborsClassifier(n_neighbors=20)# n = 5 and n = 3 accuracy 0.6,  n=20 accuracy 0.56\n",
        "#lr = LogisticRegression() #accuracy 0.27\n",
        "\n",
        "model = dtc\n",
        "model.fit(x_train_val, y_train_val)\n",
        "y_pred = model.predict(x_val)\n",
        "acc = accuracy_score(y_val, model.predict(x_val))#soon võiks ka olla ypred\n",
        "print(acc)\n",
        "\n",
        "#rf 90.1 kui limit/howMany oli 50. 89 kui limit 10 ja 90.2 kui limit on 100. aga randoomsus on ka mängus. 30-ga 90.5, 89.7, 89.9\n",
        "#dtc 89 vist kui limit/howmany 50 ja sama kui howmany on 10. 90.3 kui howmany on 100. 3+-ga 89.6, 90.0, 89.5\n"
      ],
      "metadata": {
        "id": "DRNVJczxRJJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training on the whole trainset, predicting the results on testset and saving the predictions"
      ],
      "metadata": {
        "id": "P2Av6dxsQsCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=train[columnsX]\n",
        "y_train=train[\"type\"]\n",
        "\n",
        "x_test=test[columnsX]\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "1XQAMQHfiwl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING RESULTS TO FILE\n",
        "predictions_df = pd.DataFrame(y_pred, columns = [\"type\"])\n",
        "df_pred = test[[\"id\"]]\n",
        "#df_pred[\"type\"] = predictions_df[\"type\"] #rf\n",
        "df_pred.insert(1, \"type\", y_pred) #dtc\n",
        "df_pred.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "xyOTAZ1-WbCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##code to tune the hyperparameters for decision tree and random forest"
      ],
      "metadata": {
        "id": "a8CWS4V1PYmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rf. 4 min läks\n",
        "ne = [10, 50] #n_estimators                  1\n",
        "cr = [\"gini\", \"entropy\"] #criterion   2 \n",
        "md = [None, 100, 10] #max_depth                3\n",
        "mss = [2, 3] #min_samples_split                   4\n",
        "msl = [1, 10] #min_samples_leaf                5\n",
        "mf = [\"log2\", \"sqrt\"]#max_features          6\n",
        "\n",
        "rfTuning = pd.DataFrame(columns=[\"ne\", \"cr\",\"md\", \"mss\",\"msl\", \"mf\", \"accuracy\"])\n",
        "\n",
        "bestModel=None\n",
        "maxAcc=0.0\n",
        "maxAccParams=[]\n",
        "for i1 in ne:\n",
        "  print(i1)\n",
        "  for i2 in cr:\n",
        "    print(i2)\n",
        "    for i3 in md:\n",
        "      for i4 in mss:\n",
        "        for i5 in msl:\n",
        "          for i6 in mf:\n",
        "            model = RandomForestClassifier(random_state=5, n_estimators=i1, criterion=i2, max_depth=i3, min_samples_split=i4, min_samples_leaf=i5, max_features=i6)\n",
        "            model = model.fit(x_train_val, y_train_val)\n",
        "            acc = accuracy_score(y_val, model.predict(x_val))\n",
        "\n",
        "            rfTuning = rfTuning.append({\"md\": i1, \"mss\": i2, \"msl\": i3, \"mwfl\": i4, \"mf\": i5, \"c\": i6, 'täpsus': acc }, ignore_index=True)\n",
        "            if(acc>maxAcc):\n",
        "              maxAcc=acc\n",
        "              maxAccParams=[i1, i2, i3, i4, i5, i6]\n",
        "              bestModel=model\n",
        "\n",
        "p=maxAccParams\n",
        "print(\"The best model was with parameters:  \")\n",
        "print(\"n_estimators=\"+str(p[0])+\", criterion=\"+str(p[1])+\", max_depth=\"+str(p[2])+\", min_samples_split=\"+str(p[3])+\", min_samples_leaf=\"+str(p[4])+\", max_features=\"+str(p[5]))\n",
        "print(\"and accuracy: \"+str(maxAcc))\n",
        "\n",
        "rfTuning.to_csv(\"rf_mudelite_katsed_limit50.csv\", index=False)"
      ],
      "metadata": {
        "id": "L13aCi9-a4HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from hw 5?\n",
        "md = [2, 4, 5, 10]#max_depth       1\n",
        "mss = [2, 3, 4, 6]#min_samples_split      2\n",
        "msl = [1, 2, 7]#min_samples_leaf    3\n",
        "mwfl = [0.0, 0.01, 0.05]#min_weight_fraction_leaf   4\n",
        "mf = [1, 2, None, \"sqrt\"]#max_features    5\n",
        "c = [\"gini\", \"entropy\"]#criterion   6\n",
        "\n",
        "\n",
        "dtcTuning = pd.DataFrame(columns=[\"md\", \"mss\",\"msl\", \"mwfl\",\"mf\", \"c\", \"täpsus\"])\n",
        "\n",
        "bestModel=None\n",
        "maxAcc=0.0\n",
        "maxAccParams=[]\n",
        "for i1 in md:\n",
        "  for i2 in mss:\n",
        "    for i3 in msl:\n",
        "      for i4 in mwfl:\n",
        "        for i5 in mf:\n",
        "          for i6 in c:\n",
        "            model = DecisionTreeClassifier(random_state=5, max_depth=i1, min_samples_split=i2, min_samples_leaf=i3, min_weight_fraction_leaf=i4, max_features=i5, criterion=i6)\n",
        "            model = model.fit(x_train_val, y_train_val)\n",
        "            acc = accuracy_score(y_val, model.predict(x_val))\n",
        "\n",
        "            dtcTuning = dtcTuning.append({\"md\": i1, \"mss\": i2, \"msl\": i3, \"mwfl\": i4, \"mf\": i5, \"c\": i6, 'täpsus': acc }, ignore_index=True)\n",
        "            if(acc>maxAcc):\n",
        "              maxAcc=acc\n",
        "              maxAccParams=[i1, i2, i3, i4, i5, i6]\n",
        "              bestModel=model\n",
        "\n",
        "p=maxAccParams\n",
        "print(\"The best model was with parameters:  \")\n",
        "print(\"max_depth=\"+str(p[0])+\", min_samples_split=\"+str(p[1])+\", min_samples_leaf=\"+str(p[2])+\", min_weight_fraction_leaf=\"+str(p[3])+\", max_features=\"+str(p[4])+\", criterion=\"+str(p[5]))\n",
        "print(\"and accuracy: \"+str(maxAcc))"
      ],
      "metadata": {
        "id": "QHFOcBiBU56a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}